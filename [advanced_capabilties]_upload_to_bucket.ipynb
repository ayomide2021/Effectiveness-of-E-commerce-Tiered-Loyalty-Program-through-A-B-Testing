{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayomide2021/Effectiveness-of-E-commerce-Tiered-Loyalty-Program-through-A-B-Testing/blob/main/%5Badvanced_capabilties%5D_upload_to_bucket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions (only run once)"
      ],
      "metadata": {
        "id": "ek2JOaMViv9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                                        # UPDATED: needed to check dirs & walk\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List\n",
        "\n",
        "def unzip_file(folder_path):\n",
        "    zip_path = folder_path\n",
        "\n",
        "    # Destination directory\n",
        "    extract_to = '/content/'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    print(f\"Unzipped to: {extract_to}\")\n",
        "\n",
        "def upload_file_to_gcs(file_path, bucket_name, destination_blob_name, credentials):\n",
        "    \"\"\"\n",
        "    Uploads a file or all files in a folder to a specified GCS bucket\n",
        "    using service account credentials.\n",
        "\n",
        "    - If `file_path` is a folder, uploads all contained files, then prints:\n",
        "        gs://<bucket>/<destination_blob_name>/ (N files uploaded)\n",
        "      and returns \"gs://<bucket>/<destination_blob_name>/\"\n",
        "\n",
        "    - Otherwise, uploads the single file and returns its gs:// URI.\n",
        "    \"\"\"\n",
        "\n",
        "    # —– parse credentials —–\n",
        "    if isinstance(credentials, str):\n",
        "        credentials = json.loads(credentials)\n",
        "\n",
        "    creds = service_account.Credentials.from_service_account_info(credentials)\n",
        "    client = storage.Client(credentials=creds, project=credentials.get(\"project_id\"))\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    # —– handle directory upload —–\n",
        "    if os.path.isdir(file_path):                                   # UPDATED: detect folder\n",
        "        file_count = 0                                             # UPDATED: initialize counter\n",
        "        for root, dirs, files in os.walk(file_path):               # UPDATED: grab dirs so we can prune\n",
        "            if \".ipynb_checkpoints\" in dirs:\n",
        "                dirs.remove(\".ipynb_checkpoints\")                  # UPDATED: skip Colab checkpoint folders\n",
        "            for filename in files:\n",
        "                local_path = os.path.join(root, filename)\n",
        "                rel_path = os.path.relpath(local_path, start=file_path)\n",
        "                blob_name = f\"{destination_blob_name.rstrip('/')}/{rel_path}\"\n",
        "                try:\n",
        "                    bucket.blob(blob_name).upload_from_filename(local_path)\n",
        "                    file_count += 1                               # UPDATED: count each upload\n",
        "                except Exception as e:\n",
        "                    print(f\"Error uploading '{local_path}':\", e)\n",
        "        # build and print only the directory URI + count\n",
        "        dir_uri = f\"gs://{bucket_name}/{destination_blob_name.rstrip('/')}/\"  # UPDATED\n",
        "        print(f\"{dir_uri} ({file_count} files uploaded)\")                    # UPDATED\n",
        "        return dir_uri                                                        # UPDATED\n",
        "\n",
        "    # —– single-file upload (original) —–\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    try:\n",
        "        blob.upload_from_filename(file_path)\n",
        "        gs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
        "        return gs_uri                                                       # UPDATED: return single-file URI\n",
        "    except Exception as e:\n",
        "        print(\"Error uploading file:\", e)\n",
        "        return None\n",
        "\n",
        "def validate_folder_structure(root_path: Path) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    root_path should be the <data_row_id> folder.\n",
        "    Validates that:\n",
        "      <data_row_id>/\n",
        "        ├─ data/           → must exist, not empty, only .npy/.csv\n",
        "        ├─ outputs/        → must exist, not empty, only .html\n",
        "        ├─ scripts/        → exactly data_gen.py and viz.py (no nesting)\n",
        "    Returns (is_valid, list_of_error_messages).\n",
        "    \"\"\"\n",
        "    errors: List[str] = []\n",
        "    expected_dirs = {\"data\", \"scripts\", \"outputs\"}\n",
        "    actual_dirs   = {p.name for p in root_path.iterdir() if p.is_dir()}\n",
        "\n",
        "    # 1) Top-level directories\n",
        "    missing = expected_dirs - actual_dirs\n",
        "    extra   = actual_dirs - expected_dirs\n",
        "    if \".ipynb_checkpoints\" in extra:\n",
        "        extra.remove(\".ipynb_checkpoints\") #added for colab\n",
        "    for d in missing:\n",
        "        errors.append(f\"Missing directory: {d}/\")\n",
        "    for d in extra:\n",
        "        errors.append(f\"Unexpected directory: {d}/\")\n",
        "\n",
        "    # --- data/ checks (unchanged) ---\n",
        "    data_dir = root_path / \"data\"\n",
        "    if not data_dir.exists() or not data_dir.is_dir():\n",
        "        errors.append(\"Missing directory: data/\")\n",
        "    else:\n",
        "        entries = list(data_dir.iterdir())\n",
        "        if not entries:\n",
        "            errors.append(\"data/ must not be empty\")\n",
        "        if len(entries) < 2:\n",
        "            errors.append(\"data/ must have 2 or data CSVs or NPYs files to be complex enough\")\n",
        "        for child in entries:\n",
        "            if \".ipynb_checkpoints\" in child.name:\n",
        "                continue\n",
        "            if child.suffix.lower() not in {\".npy\", \".csv\"}:\n",
        "                errors.append(\n",
        "                    f\"Invalid file in data/ → {child.name} (must be .npy or .csv)\"\n",
        "                )\n",
        "\n",
        "    # --- outputs/ checks (NEW block) ---\n",
        "    outputs_dir = root_path / \"outputs\"\n",
        "    if not outputs_dir.exists() or not outputs_dir.is_dir():\n",
        "        errors.append(\"Missing directory: outputs/\")\n",
        "    else:\n",
        "        out_entries = [p for p in outputs_dir.iterdir() if p.name != \".ipynb_checkpoints\"]\n",
        "        if not out_entries:\n",
        "            errors.append(\"outputs/ must not be empty\")\n",
        "        for child in out_entries:\n",
        "            if \".ipynb_checkpoints\" in child.name:\n",
        "                continue\n",
        "            if child.suffix.lower() not in {\".html\"}:\n",
        "                errors.append(\n",
        "                    f\"Invalid file in outputs/ → {child.name} (must be .html)\"\n",
        "                )\n",
        "\n",
        "    # helper for scripts/\n",
        "    scripts = root_path / \"scripts\"\n",
        "    if scripts.exists():\n",
        "        found = {p.name for p in scripts.iterdir() if p.is_file()}\n",
        "        want  = {\"data_gen.py\", \"viz.py\"}\n",
        "        for f in want - found:\n",
        "            errors.append(f\"Missing script file in scripts/ → {f}\")\n",
        "        for f in found - want:\n",
        "            errors.append(f\"Unexpected script file in scripts/ → {f}\")\n",
        "        for p in scripts.iterdir():\n",
        "            if \".ipynb_checkpoints\" in p.name:\n",
        "                continue\n",
        "            if p.is_dir():\n",
        "                errors.append(f\"Nested directory not allowed in scripts/ → {p.name}/\")\n",
        "\n",
        "    return (len(errors) == 0), errors"
      ],
      "metadata": {
        "id": "WYD9uLrJGMU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put all the data in the following structure and upload using the calls below\n",
        "\n",
        "```\n",
        "<data row id>/\n",
        "├── data/\n",
        "    ├── sample.npy    \n",
        "    ├── dataframe2.csv\n",
        "    └── dataframe.csv          #Generated .csv and/or npy files\n",
        "\n",
        "├── scripts/\n",
        "    ├── data_gen.py            #Data generation script\n",
        "    └── viz.py                 #Visualization script\n",
        "\n",
        "├── outputs/\n",
        "    └── golden_image.html      #Interactive html coming out of the plotly fig.write_html(\"./name_me_something_useful.html\").\n",
        "```"
      ],
      "metadata": {
        "id": "NYl-jtrTiRO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'advanced_capabilities'\n",
        "credentials = {\n",
        "    \"type\": \"service_account\",\n",
        "    \"project_id\": \"dataoperations-449123\",\n",
        "    \"private_key_id\": \"5a12330bb6b15e3b3d72253577a7a0108e20234c\",\n",
        "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCsFAblUsg2BrLe\\nrB8KyoT0Z9XKx/fB7a6TXDFM9q2ebBV5O0Iqztgkp4Eu7M+yumwq4yUxSstRPIDE\\nLS71//G0LPq0mXRgHSwlbSwQ+itE19gVMKx+z58duX+BZ/aJ3BJdQs+/XnWhhmo0\\nYXwjFfUAUjEMGVjzH6BtvZadMf69EfD0HmjH0BBNTEtSivOwiT/Rb4nCPeFn1Yay\\nxBmIf3CmZDgtBR4nKa2/zv8nQbGs1ORKECjeQ3X3oLMj4dtCgft0CrxHFfZtnxBw\\naThQXP+RBgwO14DkzxXh7jaW+ePhtOO/MPFQr8eYLM+3/HuvLAYRjmZVwinIdVsW\\n0lBBkQsvAgMBAAECggEAAdwH8XIO9ptLhlTgoDKgc/rfRSnnoOSoCBc5848pQL01\\nEJjpIrwWrkBR70DhEBksBanSUBFzCwJKKrrv3ZN4RPsrcy72ImFbH17QcbDr+mCr\\niOcV8ugP2+r4pSklCxS7ht7e9QsMEffGFAgHQGKcKCm7Nlp1XjZpFGaB4MCWjewS\\nj0zptuwEPQ8nNkSm5EloLIOjEB/xOeuWxQaz079MYIoNi0qtMPPIMIEjTASLdVbX\\nbV8dkhr4QLSppHOipW5gNfgdFZYx7AkNsaXU38aF3OJcGFsmTH9CRc6X/D+9/5s2\\nf+ZRVY9ZxC3VvAPlKWukoJOBKpWlHaQa4pmdaAlBeQKBgQDjbh+aE5uqG316A4G6\\nnMMPU+uGHyQFNAbBPYRhZ9xHdEivWDL535DJMddUvJZiGJaopdS0UqZvnCBnVwBx\\nvTh5zPjzUgr4EWdHmiMQ9vy5LrO+AZ0I4vFH/Mw/vFyO7JphKiW2WLIMWhCAGeNR\\nd7FKfOlNOxhDXnOzL61hmrAgDQKBgQDBsdpK5VGMe+oMJmHx0Mrd4m/aFK0Si9vT\\nC08r2viR3d81o4HvfkM+PhtOnel1NwR7ClW4NPO5C7eMiUF1R+Zfnv/olil2k9KF\\n/7oABIBuy4ED+o9TDBNQcA+/mrssRf+mZnqgTLD60PF4hNs3MCz/UKOZwj3ahsXS\\nTXqsRAcNKwKBgQC+i3k2oc79ymJqY53BN63Fnc3qZRTMtzYhmOTcpyPLSgXLwt2S\\ntFid+IAsooRsU6WGTsnS+pwvTmNnsWDR6nLyuWSql8ZQ6GPbfax7fjNnA1Xcj4V9\\n+IWmhoqpq6rwpBMD7UgDanRiHONOMGJf0yRbACSHbEd7yhqtufhNmkiN0QKBgHZ+\\ntJnPEoWMJRHLxW2nGwSFC4Jx1bOb5h6FM6kTq6+o+W2aGtqF9uM0IYaF6pWv08jl\\n+KzZkCSre125dlcmZlQnNE141+LX6hnZ6VMrbdraGpJxjY7zuzkhZTEFmu4p4I1O\\n8kPwxvCaNK9TL7zidxS2o29kOmzeuFTA24RZlarDAoGASj1jqlkg/TGqDu1azKX5\\nksr9nXjYuVPP4+n9tf0OoDgdDxRDfijgdPcL3tJae1IsawYHDsa0NciU25QDocbh\\nPn+3voqNoO0NreGaqZL7MycV6CDAf1Qf38dIe2yLpYKKtddRfhHDF5WBRS8XjBLP\\nwQzLDIijESyckEIrYZHq1WM=\\n-----END PRIVATE KEY-----\\n\",\n",
        "    \"client_email\": \"adv-cap@dataoperations-449123.iam.gserviceaccount.com\",\n",
        "    \"client_id\": \"108132512627608538619\",\n",
        "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/adv-cap%40dataoperations-449123.iam.gserviceaccount.com\",\n",
        "    \"universe_domain\": \"googleapis.com\"\n",
        "}\n",
        "\n",
        "\n",
        "# Assume `bucket_name` and `credentials` are preconfigured\n",
        "# Find this data row id in the datarow details: https://docs.labelbox.com/docs/label-data#data-row-information-panel\n",
        "datarow_id = \"cmce5hjxl10480798zr1e1j6u\"\n",
        "\n",
        "ok, errors =  validate_folder_structure(Path(\"./\" + datarow_id))\n",
        "if not ok:\n",
        "    for e in errors:\n",
        "        print(\"ERROR: \", e)\n",
        "else:\n",
        "\n",
        "    # 1) Upload generated data files from the data folder\n",
        "    print(\"data folder URIs: \", upload_file_to_gcs(\n",
        "        file_path=f\"{datarow_id}/data\",\n",
        "        bucket_name=bucket_name,\n",
        "        destination_blob_name=f\"{datarow_id}/data\",\n",
        "        credentials=credentials\n",
        "    ))\n",
        "    print(\"**\"*50)\n",
        "\n",
        "    # 2) Upload scripts from the scripts folder\n",
        "    print(\"scripts folder URIs: \", upload_file_to_gcs(\n",
        "        file_path=f\"{datarow_id}/scripts\",\n",
        "        bucket_name=bucket_name,\n",
        "        destination_blob_name=f\"{datarow_id}/scripts\",\n",
        "        credentials=credentials\n",
        "    ))\n",
        "    print(\"**\"*50)\n",
        "\n",
        "    # 3) Upload outputs from the outputs folder\n",
        "    print(\"outputs folder URIs: \", upload_file_to_gcs(\n",
        "        file_path=f\"{datarow_id}/outputs\",\n",
        "        bucket_name=bucket_name,\n",
        "        destination_blob_name=f\"{datarow_id}/outputs\",\n",
        "        credentials=credentials\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNSbBvglPG-r",
        "outputId": "83ef69a0-0423-46cf-8e5e-b2808cf20306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/data/ (2 files uploaded)\n",
            "data folder URIs:  gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/data/\n",
            "****************************************************************************************************\n",
            "gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/scripts/ (2 files uploaded)\n",
            "scripts folder URIs:  gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/scripts/\n",
            "****************************************************************************************************\n",
            "gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/outputs/ (1 files uploaded)\n",
            "outputs folder URIs:  gs://advanced_capabilities/cmce5hjxl10480798zr1e1j6u/outputs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SINGLE FILE UPLOADS"
      ],
      "metadata": {
        "id": "CO3pNvvNiD0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"data folder URIs: \", upload_file_to_gcs(\n",
        "    file_path=f\"{datarow_id}/data/sample.csv\",\n",
        "    bucket_name=bucket_name,\n",
        "    destination_blob_name=f\"{datarow_id}/data/sample.csv\",\n",
        "    credentials=credentials\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEMYVEOoiKzF",
        "outputId": "682d63cd-4626-43ce-a429-5e41a6219f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error uploading file: [Errno 2] No such file or directory: 'cmce5hjxl10480798zr1e1j6u/data/sample.csv'\n",
            "data folder URIs:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OINGn8mu5ykh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}